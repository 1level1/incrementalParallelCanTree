spark-submit --class "CanTreeMain" --master local --driver-memory 4g --executor-memory 4g --conf 'spark.executor.extraJavaOptions=-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:/home/lev/Documents/teza/cantree/log.gc.driver' --conf 'spark.driver.extraJavaOptions=-XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -Xloggc:/home/lev/Documents/teza/cantree/log.gc' --conf "" target/scala-2.11/cantree_2.11-0.1.jar



export log4j_setting="-Dlog4j.configuration=file:log4j.properties"

spark-submit --class "CanTreeMain" --master local --driver-memory 4g --executor-memory 4g --conf "spark.executor.extraJavaOptions=-Dlog4j.configuration=file:src/main/resources/log4j_file_executor.properties -Dsun.io.serialization.extendedDebugInfo=true -Xss:10m" --conf "spark.driver.extraJavaOptions=-Dlog4j.configuration=file:src/main/resources/log4j_file.properties -Dsun.io.serialization.extendedDebugInfo=true -Xss:10m" --files /home/lev/Documents/teza/cantree/src/main/resources/log4j_file_executor.properties,/home/lev/Documents/teza/cantree/src/main/resources/log4j_file.properties target/scala-2.11/cantree_2.11-0.1.jar




spark-submit --class "CanTreeMain" --master local --driver-memory 100g --executor-memory 100g --conf "spark.executor.extraJavaOptions=-Dlog4j.configuration=file:src/main/resources/log4j_file_executor.properties -Dsun.io.serialization.extendedDebugInfo=true -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -Xloggc:/home/lev/Documents/teza/cantree/log_executor.gc" --conf "spark.driver.extraJavaOptions=-Dlog4j.configuration=file:src/main/resources/log4j_file.properties -Dsun.io.serialization.extendedDebugInfo=true -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -Xloggc:/home/lev/Documents/teza/cantree/log_driver.gc" --files /home/lev/Documents/teza/cantree/src/main/resources/log4j_file_executor.properties,/home/lev/Documents/teza/cantree/src/main/resources/log4j_file.properties target/scala-2.11/cantree_2.11-0.1.jar


-Xloggc:/home/lev/Documents/teza/cantree/log_executor.gc
-Xloggc:/home/lev/Documents/teza/cantree/log_driver.gc

-XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark



spark-submit --class "CanTreeMain" --master local --driver-memory 4g --executor-memory 4g --conf "spark.executor.extraJavaOptions=-Dlog4j.configuration=file:src/main/resources/log4j_file_executor.properties -Xss10m" --conf "spark.driver.extraJavaOptions=-Dlog4j.configuration=file:src/main/resources/log4j_file.properties -Xss10m" --files /home/lev/Documents/teza/cantree/src/main/resources/log4j_file_executor.properties,/home/lev/Documents/teza/cantree/src/main/resources/log4j_file.properties target/scala-2.11/cantree_2.11-0.1.jar --num-partitions 2 --min-support 0.1 --in-file-path OnlineRetail.csv


val df1 = df.map(t=> ((patter findAllIn t.toString).toArray.slice(1,3)))


import org.apache.spark.Partitioner
import org.apache.spark.sql.{Row, SparkSession}
import java.{util => ju}
import scala.util.matching.Regex

import org.apache.spark.rdd.RDD
import org.apache.spark.rdd.RDD._

val p = "/home/lev/Documents/teza/dataset"
//val fName = "T15I5D1000N10"
val fName = "T15I5D10000N100"
val suffix = ".data"
//val spark = SparkSession.builder.appName("CAN_TREE").config("spark.master", "local").getOrCreate()
val df = spark.read.text(p+"/"+fName+suffix)
val patter = new Regex("(\\d)+")
val df1 = df.map(t=> ((patter findAllIn t.toString).toArray.slice(1,3)))
val df2 = df1.withColumn("InvoiceNo",$"value"(0)).withColumn("StockCode",$"value"(1))
val df3 = df2.drop("value")
//df3.write.csv("/home/lev/Documents/teza/dataset/T1I5D10.csv")
df3.write.csv("hdfs://localhost:9000/"+fName+".csv")

sudo -s sysctl -w vm.oom-kill = 0


tmux a
^-b 
x2goclient



spark-submit --class "CanTreeMain" --master spark://cn41-ib:7077 --driver-memory 60g --executor-memory 60g --total-executor-cores 240 --conf "spark.executor.extraJavaOptions=-Dlog4j.configuration=file:src/main/resources/log4j_file_executor.properties" --conf "spark.driver.extraJavaOptions=-Dlog4j.configuration=file:src/main/resources/log4j_file.properties" --files /RG/rg-gudes/kulev/cantree/src/main/resources/log4j_file_executor.properties,/RG/rg-gudes/kulev/cantree/src/main/resources/log4j_file.properties target/scala-2.11/cantree_2.11-0.1.jar --num-partitions 1 --min-support 0.1 --in-file-path `realpath ../dataset/T20I10D100000K.csv` 

./gen lit -ntrans 10 -tlen 15 -nitems 1 -patlen 5 -fname ../T1I5D10.data
D: number of sequences in the dataset 
C: average number of itemsets per sequence 
T: average number of items per itemset 
S: average number of itemsets in potentially frequent sequences. 
I: average size of itemsets in potentially frequent sequences 
N: number of different items in the dataset

val pairs = sc.parallelize(Array(("a", 3), ("a", 1), ("b", 7), ("a", 5)))
import scala.collection.mutable.ListBuffer
var l = new ListBuffer[Int]()
pairs.aggregateByKey(l)(l+=_)((l1,i1)=> {l1+=i1},(l1,l2)=>{l1++l2})




  import org.apache.spark.{Partitioner, SparkException}
  import org.apache.spark.sql.types.{IntegerType, StructField, StructType}
import scala.collection.mutable
import scala.collection.mutable.ListBuffer

   type Sorter[T] = (T, T) => Boolean
    type StringSorter = Sorter[String]
type IntSorter = Sorter[Int]
def intSorter : IntSorter = (i1,i2) => i1<i2

  val customSchema = StructType(Array(
  StructField("InvoiceNo", IntegerType, true),
  StructField("StockCode", IntegerType, true))
  )
  val fileName = "/RG/rg-gudes/kulev/T15I5D1000N10.csv"
val df = spark.read.format("csv").option("header", "true").schema(customSchema).load(fileName.asInstanceOf[String])
val dfGrouped = df.groupBy("InvoiceNo").agg(collect_list(col("StockCode")))
import org.apache.spark.{HashPartitioner, Partitioner, SparkContext, SparkException}
val partitioner = new HashPartitioner(1)
val transactions = dfGrouped.rdd.map(t=>t(1).asInstanceOf[mutable.WrappedArray[Int]].toArray)
import levko.cantree.utils.{CanTree, CanTreeFPGrowth}
val model = new CanTreeFPGrowth().setMinSupport(0.01).setPartitioner(partitioner)


val fileName1 = "/home/lev/Documents/teza/cantree/smallTest1.csv"
val fileName2 = "/home/lev/Documents/teza/cantree/smallTest2.csv"
import org.apache.spark.{Partitioner, SparkException}
import org.apache.spark.sql.types.{IntegerType, StructField, StructType}
import scala.collection.mutable
import scala.collection.mutable.ListBuffer
type Sorter[T] = (T, T) => Boolean
type StringSorter = Sorter[String]
type IntSorter = Sorter[Int]
def intSorter : IntSorter = (i1,i2) => i1<i2
val customSchema = StructType(Array(
	StructField("InvoiceNo", IntegerType, true),
	StructField("StockCode", IntegerType, true))
)

val df1 = spark.read.format("csv").option("header", "true").schema(customSchema).load(fileName1)
val dfGrouped1 = df1.groupBy("InvoiceNo").agg(collect_list(col("StockCode")))
val transactions1 = dfGrouped1.rdd.map(t=>t(1).asInstanceOf[mutable.WrappedArray[Int]].toArray)
val df2 = spark.read.format("csv").option("header", "true").schema(customSchema).load(fileName2)
val dfGrouped2 = df2.groupBy("InvoiceNo").agg(collect_list(col("StockCode")))
val transactions2 = dfGrouped2.rdd.map(t=>t(1).asInstanceOf[mutable.WrappedArray[Int]].toArray)

val partitioner = new HashPartitioner(1)

val model = new CanTreeFPGrowth().setMinSupport(minSupport).setPartitioner(partitioner)
val canTrees1 = model.genCanTrees(transactions1,intSorter)
val canTrees2 = model.genCanTrees(transactions2,intSorter)
val r = canTrees1.fullOuterJoin(canTrees2).map{ case (part,(Some(tree1),Some(tree2))) => (part,tree1.merge(tree2))
						case (part,(Some(tree1),_)) => (part,tree1)
						case (part,(_,Some(tree2))) => (part,tree2)}


def createIterationFiles(fileName : String,iterNum : Int,outputdir : String) : Unit = {
	import org.apache.spark.{Partitioner, SparkException}
	import org.apache.spark.sql.types.{IntegerType, StructField, StructType}
	import scala.collection.mutable
	import scala.collection.mutable.ListBuffer

	type Sorter[T] = (T, T) => Boolean
	type StringSorter = Sorter[String]
	type IntSorter = Sorter[Int]
	def intSorter : IntSorter = (i1,i2) => i1<i2

	val customSchema = StructType(Array(
		StructField("InvoiceNo", IntegerType, true),
		StructField("StockCode", IntegerType, true))
	)
	val df = spark.read.format("csv").option("header", "true").schema(customSchema).load(fileName.asInstanceOf[String])
	val boundaries = df.agg(min("InvoiceNo"),max("InvoiceNo"))
	val maxRow = df.agg(max("InvoiceNo")).take(1)(0).getInt(0)
	val mid = math.floor(0.5*maxRow)
	df.filter(df("InvoiceNo") < mid).write.csv(outputdir+"baseDF.csv")
	val iterSize = math.ceil(mid/iterNum).toInt
	for (i <- 0 to iterNum-1) {
		val currStart = mid+i*iterSize
		val currEnd = mid+(i+1)*iterSize
		df.filter(df("InvoiceNo")>=currStart && df("InvoiceNo") < currEnd).write.csv(outputdir+i+".csv")
	}
}

for ((i=0; i<20; i++)) 
do 
for ((j=0;j<5;j++))
do
echo "$i $j"
cat /RG/rg-gudes/kulev/T15I5D10000N100/$((5*${i}+${j})).csv >> /RG/rg-gudes/kulev/T15I5D10000N100_20SPLITS/${i}.csv
done
done




cnt=1
tot=len(res)
ffty = int(tot/2)
leap=int(ffty/20)
with open("/home/lev/Documents/teza/dataset/kosarak_20splits/base.csv","w") as fout:
 for r in res[0:ffty]:
  for l in set(r.rstrip().split(' ')):
   fout.write(str(cnt)+","+l+"\n")
  cnt+=1

for i in range(0,20):
 with open("/home/lev/Documents/teza/dataset/kosarak_20splits/"+str(i)+".csv","w") as fout:
  for r in res[ffty+(i*leap):ffty+((i+1)*leap)]:
   for l in set(r.rstrip().split(' ')):
    fout.write(str(cnt)+","+l+"\n")
   cnt+=1
 


spark-submit --class CanTreeMain --master local --driver-memory 8g --executor-memory 8g --num-executors 2 --executor-cores 3 --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:src/main/resources/log4j_file.properties target/scala-2.11/cantree_2.11-0.1.jar --num-partitions 1 --min-support 0.1 --in-file-list-path /RG/rg-gudes/kulev/kosarak_20splits/filesList.txt --app-name kosarak_1_40_4_20g_0_1


rsync -av -e 'ssh -o "Proxycommand ssh kulev@sheshet.cslab.openu.ac.il -W %h:%p"' target/scala-2.11/cantree_2.11-0.1.jar kulev@login.hpc.pub.lan:~/work/cantree/target/scala-2.11/cantree_2.11-0.1.jar

ssh -L 18081:localhost:18080 kulev@sheshet.cslab.openu.ac.il


var itemsHash : mutable.HashMap[Item,Int] = mutable.HashMap.empty
for (i in 0 to 1000)
    itemHash(i)=i

def runRand(itemsHash : mutable.HashMap[Int,Int]) : Unit = {
    val r = scala.util.Random
    1 to 100000 foreach( i => {
        val item : Int = i.asInstanceOf[Int]
        itemsHash(item)
        }
    )
}

def runRandIm[Item:ClassTag](itemsHash : immutable.Map[Item,Int]) : Unit = {
    val r = scala.util.Random
    0 to 100000 foreach( i => itemsHash(r.nextInt(100000)))
}


var subs : List[Set[Int]] = List.empty
itemsHash.map( i => Set(i._1) ::subs )
