\section{Conclusions}
\iffalse
For a single computation of frequent items, the benchmark for performance and memory for IPFIM, is PFP. This is because IPFIM is using similar techniques, and FP tree is the optimal structure for this purpose (except some variations mentioned in previous sections, e.g. optimal sharding).
As already mentioned in the \hyperref[sec:discussion]{\textit{discussion}} section, when there is a relatively equal ratio between reading a dataset and computation time of frequent item sets, IPFIM with the suggested improvements out performs PFP. However, for large FIS computation time, this advantage is negligible in total.

Using a canonical order approach, as in Cantree, was almost not practical for large data sets, nor for small min support calculations. The improvement of using a semi-frequency and pre-min support limitation, provides the best balance , and provides best performance.

For future work, it is interesting to enhance PFP to use "smart" grouping. For example trying to use greedy set cover to find groups for of frequent itemsets.
\fi

In this thesis we have tested and compared 3 new techniques for incremental parallel mining of frequent itemsets.A major achievement of this thesis, is that IPFIM-Improved proved to have good results and a simple solution and implementation by combining 4 techniques - AFPIM, CPTree, CanTree and PFP. The Set-Cover algorithm, although has rational for optimized groups, the overhead of using groups as map instead of int-hash proved to be very resource dependent and inefficient. IPFIM is mostly useful for moderate min-support and database size.

For future work, we would suggest to test IPFIM-Improved with full implementation using CPTree~\autoref{par:cptree} and AFPIM~\autoref{par:AFPIM} suggested techniques and adjustments.
