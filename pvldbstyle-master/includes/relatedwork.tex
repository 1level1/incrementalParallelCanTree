\section{RELATED WORK}

\subsection{Incremental Frequent Itemsets Mining}
The definition of an Incremental update, is to recompute outputs which depend on the incoming inputs only, without recomputing the whole data.

The challenge while performing incremental updates for frequent items mining, is a non consistent frequency order. Several algorithms such as AFPIM~\cite{koh2004efficient}, EFPIM~\cite{li2006fast} and FUFP-tree~\cite{hong2008incrementally} are keeping an updated frequency based trees, by reordering branches where frequency has changed. 

\paragraph{Canonical-Tree}
The work of~\cite{leung2005cantree} presented a Canonical Tree (CanTree) which preserves the frequency descending structure as in FP Growth mining, by relying on a predefined order, which will not affect the tree structure and correctness.

The predefined order, creates some nice properties, as described below:

\begin{enumerate}
	\item Items are arranged according to a canonical order, which is a fixed
global ordering.
	\item The ordering of items is unaffected by the changes in frequency
caused by incremental updating.
	\item The frequency of a node in the CanTree is at least as high as the sum
of frequencies of all its children.
\end{enumerate}

Since CanTree preserves same feature as the FP-Tree for mining FIS, the mining is done in the same fasion as the original FP-Growth algorithm.

An example of a CanTree is presented in \autoref{tab:table1} and \autoref{fig:CanTreeExample}

\begin{table}[h!]
  \begin{center}
    \caption{Consider the following database:}
    \label{tab:table1}
    \begin{tabular}{l|c|l} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
      \textbf{} & \textbf{TID} & \textbf{Contents}\\
      \hline
      Original database (DB) & t\textsubscript{1} & {a, d, b, g, e, c}\\
       & t\textsubscript{2} & {d, f, b, a, e}\\
        & t\textsubscript{3} & {a}\\
	   & t\textsubscript{4} & {d, a, b}\\
       The first group of insertions (db1) & t\textsubscript{5} & {a, c, b}\\
        & t\textsubscript{6} & {c, b, a, e}\\
       The second group of insertions (db2) & t\textsubscript{7} & {a, b, c}\\
        & t\textsubscript{8} & {a, b, c} \\
    \end{tabular}
  \end{center}
\end{table}



\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figures/CanTreeExample}
  \caption{The CanTree after each group of transactions is added}
  \label{fig:CanTreeExample}
\end{figure}



\paragraph{CP-Tree}
\label{par:cptree}
	The work of~\cite{tanbeer2009efficient} proposes an improvement to CanTree, called CompactPattern-Tree, and discusses the memory and computation limitations of CanTree for large incremental Databases. The issues are caused due to un-efficient tree structure, and CP-Tree is proposing an improvement by periodically (using a proposed guideline) updating the order of the construction literals list (l-list) and rebuilding the trees. As mention in the original article and as seen by our experiments, the CanTree and CP-Tree has a similar tree size, and the difference for our test cases was 10\% in tree sizes. However as seen in our results, using semi-frequency based order, improves the mining results by 10X and more for smaller minSupport values [TODO: Add graphs].

\paragraph{AFPIM - Adjusting FP-tree Structure for Incremental Mining }
\label{par:AFPIM}
  The work by ~\cite{kohefficient}, is using a technique of pre-min support threshold for defining a pre-frequent itemset. The rational behind this approach is that to identify those items within every iteration, does not require another scan of the previous data as it is already part of the intermediate tree.  Table \ref{tab:AFPIMCases} demonstrates the different scenarios in to the possibilities when new iterations are added, $ \cup D $ is the union of all iterations on DB. $ \cup D $ needs to be scanned to re-construct the FP-tree of
$ \cup D $  only if there exist any item belonging to case 4. For cases 5 or 6, which are in-frequent in DB, these items can be ignored because the corresponding itemsets are not frequent in $ \cup D $.  For the correctness of the algorithm, 3 operations were defined:

\begin{enumerate}[label=\textbf{AFPIM.\arabic*}]
\item \label{AFPIMRemove} Remove nodes of infrequent items - Direct remove of the infrequent nodes and reattach sons to parent.
\item \label{AFPIMAdjust} Adjusting the path of nodes - a bubble-sort for maintaining frequency order.
\item Insert or remove data from FP-tree - After the tree is scanned and re-sorted based on~\ref{AFPIMAdjust}, add/remove new sorted data.
\end{enumerate}

\paragraph{AFPIM example}
\label{par:AFPIMexample}
Let the original database, DB, be illustrated in ~\ref{tab:AFPIM}. The minimum
support is 0.2 and the pre-minimum support is 0.15. Scan DB once to collect the set of
frequent or pre-frequent items: A:2, B:6, C:5, D:3, E:4, F:7, G:1, and H:1 (:indicates the
support count). The items with support counts no less than 2 (i.e. 13×0.15=1.95) are
frequent or pre-frequent items in DB. Thus, A, B, C, D, E, and F are frequent items in
DB. After sorting all the frequent or pre-frequent items in support descending order, the
result is F:7, B:6, C:5, E:4, D:3, and A:2. Accordingly, the constructed FP-tree of DB is
shown as ~\ref{fig:AFPIMBase}.
Then 5 transactions are inserted into and 3 transactions are removed from DB,
where the transaction data are shown as ~\ref{tab:AFPIMdbp} and ~\ref{tab:AFPIMdbm}. The support counts of all the items in $ db^+ $ and $ db^- $ are listed in ~\ref{tab:AFPIMFinal}. For each item X in $\cup D $,  minimum support of X can be obtained by a simple computation. The result is A:2, B:9, C:5, D:7, E:6, F:8, G:1, and H:2. In new database $\cup D $, a pre-frequent or frequent item must have support counts no less than 3 (i.e. (13+5-3)×0.15=2.25). Therefore, the frequent or pre-frequent 1-itemsets in $\cup D $, shown in frequency descending order, are B:9, F:8, D:7, E:6, and C:5.  As shown in ~\ref{tab:AFPIMFinal} , A is not a frequent or pre-frequent item in $\cup D $,.
Thus, the nodes representing A are removed from the FP-tree. The resultant FP-tree is shown as ~\ref{fig:AFMPIAfterDelete}. We will not discuss in detail the other operations, as in IPFIM improved algorithm we use a semi-frequency order, and no need to perform ~\ref{AFPIMAdjust}, as for correctness of the mining, a rescan and rebuild of the FP-Tree for case 4 in \ref{tab:AFPIMCases} is required.  However we skipped this phase for the evaluations.
  
\begin{table}[h!]
  \begin{center}
    \caption{AFPIM cases}
    \label{tab:AFPIMCases}
    \begin{tabular}{l|c|l} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
      \thead{case} & \thead{in DB} & \thead{in $\cup D $ }\\
      \hline
Case 1 &  \makecell{X is a frequent or \\ pre-frequent item} & X is a frequent item \\
Case 2 & \makecell{X is a frequent or \\ pre-frequent item} & X is a pre-frequent item \\
Case 3 & \makecell{X is a frequent or \\ pre-frequent item} & X is an infrequent item \\
Case 4 & X is an infrequent item & X is a frequent item \\
Case 5 & X is an infrequent item & X is a pre-frequent item \\
Case 6 & X is an infrequent item & X is an infrequent item \\
    \end{tabular}
  \end{center}
\end{table}
  
  
  \begin{table}[h!]
  \begin{center}
    \caption{Original Database DB for AFPIM:}
    \label{tab:AFPIM}
    \begin{tabular}{l|c|cl} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
      \thead{TID} & \thead{Items} & \thead{Frequent or pre-frequent items \\
(ordered in frequency descending order) } \\
      \hline
      1 & BDEF & FBED\\
      2 & F & F\\
       3 & ABEF & FBEA \\
	  4 & CH & C \\
       5 & BF  & FB \\
       6 & B  & B \\
       7 &  ABEF & FBEA \\
       8 & CG  & C \\
       9 &  BF & FB \\
       10 & CDE & CED \\
       11 & F  & F \\
       12 & CD & CD \\
       13 & C & C \\
    \end{tabular}
  \end{center}
\end{table}

  \begin{table}[h!]
  \begin{center}
    \caption{$ db^+$ }
    \label{tab:AFPIMdbp}
    \begin{tabular}{l|cl} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
      \thead{TID} & \thead{Itemset} \\
      \hline
       14 & BCDEF   \\
       15 & BDEF  \\
       16 & BCDG  \\
       17 & BD  \\
       18 & DH  \\
    \end{tabular}
  \end{center}
\end{table}

  \begin{table}[h!]
  \begin{center}
    \caption{$ db^-$ }
    \label{tab:AFPIMdbm}
    \begin{tabular}{l|cl} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
      \thead{TID} & \thead{Itemset} \\
      \hline
       5 & BF   \\	
       8 & CG  \\
       1 & CD  \\
    \end{tabular}
  \end{center}
\end{table}




  \begin{table}[h!]
  \begin{center}
    \caption{$ \cup D$ Frequency}
    \label{tab:AFPIMFinal}
    \begin{tabular}{l|c|l} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
      \thead{Database} & \thead{Items} & \thead{Frequent or pre-frequent items \\
(ordered in frequency descending order) } \\
      \hline
DB & A:2, B:6, C:5, D:3, E:4, F:7, G:1, H:1 & F:7, B:6, C:5, E:4, D:3, A:2\\
$ db^+ $ & B:4,C:2,D:5,E:2,F:2,G:1,H:1 & - \\
$ db^- $ & B:1,C:2,D:1,F:1,G:1 & -\\
$ \cup D$  & A:2, B:9, C:5, D:7, E:6, F:8, G:1, H:2 & B:9, F:8, D:7, E:6, C:5 \\
 \end{tabular}
  \end{center}
\end{table}


\begin{figure}
  \centering
  \includegraphics[scale=0.75]{figures/AFPIMBase}
  \caption{FP-tree of AFPIM DB}
  \label{fig:AFPIMBase}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[scale=0.75]{figures/AFMPIAfterDelete}
  \caption{FP-tree After delete of $ \cup D$}
  \label{fig:AFMPIAfterDelete}
\end{figure}


\subsection{Parallel Frequent Itemsets mining}
The difficulty in parallelizing FP-growth is to distribute iterations to parallel trees while still allowing correct mining. PFP~\cite{li2008pfp} is solving this by dividing the DB transactions to independent trees using a Group-List, where every group consists of subgroup of the original items, and redistributing iterations in the DB based on this list.
PFP~\cite{li2008pfp} has the following MapReduce stages:

\begin{steps}
\item Calculate the global frequency list F-list, by MapReduce "Work Count" manner.
\item In the second job, the the map will have the following functionality:
\begin{enumerate}
\item Sort transactions based on F-list.
\item Replace items in a transaction with the appropriate group id mapped transactions.
\end{enumerate}
	The reducer here will build the trees in a parallel manner, based on the group id of the mapping stage.
\item In the final mapping stage, every mapper will project the sub tree from a 1-item-length frequent itemset of the group, where the reducers will recursively mine those sub-trees. The parallelization in that case, can be at most equal to the number of items in the database.
\end{steps}

A more detailed description of PFP~\cite{li2008pfp} is described here \autoref{alg:pfpalg}:
\begin{algorithm}
  \caption{Highlevel description of the PFP-Growth algorithm}\label{euclid}
  \begin{algorithmic}[1]
   \label{alg:pfpalg}
    \Procedure{PFP-Growth}{}
      \State $F-List\gets$ Find global frequency list
      \State $G-list\gets$ Define a Group items
      \ForEach{$transaction$ T\textsubscript{i} $in$ DB}
        \State t\textsubscript{i}$\gets$ order by F-List frequency
		\State $G-hashed-list$\textsubscript{i} $\gets$ replace every element a\textsubscript{j} $in$ t\textsubscript{i} with Hash(g), where a\textsubscript{j}$\in g$ And $g\in G-list$
      	\ForEach{Hash(g) $\in G-hashed-list$}
      		\State $L\gets$ find its right-most location in t\textsubscript{i}
      		\State Output key'=g; value'={t\textsubscript{i}[0]…t\textsubscript{i}[L]}
      	\EndFor
      \EndFor
 	\State Group by key' = g
 	\State For each group g, build appropriate tree
 	\State For each group g, mine the generated tree.
    \EndProcedure
  \end{algorithmic}
\end{algorithm}


To better understand the distribution of transactions between groups in PFP, an example is provided in \autoref{tab:table2}. This example shows the initial state of raw transactions, sorting them frequency based, and distributing based on a G-List of single item per group - \{C\},\{E\},\{B\},\{A\}. Last lines demonstrate the output at line 9 \autoref{alg:pfpalg} of every transaction. 

\begin{table}[h!]
  \begin{center}
    \caption{A simple example of distributed FP-Growth:}
    \label{tab:table2}
    \begin{tabular}{l|c|l} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
      \textbf{} & \textbf{TID} & \textbf{Items}\\
      \hline
      Original database (DB) & t\textsubscript{1} & {A, B}\\
       & t\textsubscript{2} & {E, C}\\
        & t\textsubscript{3} & {E, A, C}\\
	   & t\textsubscript{4} & {E, C}\\
       & t\textsubscript{5} & {E, C}\\
        & t\textsubscript{6} & {B}\\
	   & t\textsubscript{7} & {A, C}\\
       & t\textsubscript{8} & {E, D, C}\\
        & t\textsubscript{9} &{E, B} \\
	   & t\textsubscript{10} & {E, B}\\
       & t\textsubscript{11} & {E, A, C}\\
        & t\textsubscript{12} & {E, C}\\
	   & t\textsubscript{13} & {B, D, C}\\
	  F-List: & A: 4, E: 9, B: 5, C: 9, D: 2 & \\
	  		\hline
	  Support=4 & Sorted transactions => [C,E,B,A] & \\
	  		\hline
	  Sorted and Filtered & t\textsubscript{1} & {B,A}\\
       & t\textsubscript{2} & {C,E}\\
        & t\textsubscript{3} & {C, E, A}\\
	   & t\textsubscript{4} & {C,E}\\
       & t\textsubscript{5} & {C,E}\\
        & t\textsubscript{6} & {B}\\
	   & t\textsubscript{7} & {C, A}\\
       & t\textsubscript{8} & {C,E}\\
        & t\textsubscript{9} &{E, B} \\
	   & t\textsubscript{10} & {E, B}\\
       & t\textsubscript{11} & {C, E, A}\\
        & t\textsubscript{12} & {C,E}\\
	   & t\textsubscript{13} & {C, B}\\
		\hline
	   G-List & \{C\},\{E\},\{B\},\{A\} &\\
		\hline
		$g\in G-List$ & Transactions & FIS\\
		\hline
		\{C\}& t\textsubscript{2},t\textsubscript{3},t\textsubscript{4},t\textsubscript{5},t\textsubscript{7},t\textsubscript{8},t\textsubscript{11},t\textsubscript{12},t\textsubscript{13}& [C]\\
		\{E\}& 		t\textsubscript{2},t\textsubscript{3},t\textsubscript{4},t\textsubscript{5},t\textsubscript{8},t\textsubscript{11},t\textsubscript{12} & [C,E]\\
		& t\textsubscript{9},t\textsubscript{10} & [E]\\
		\{B\} &t\textsubscript{1},t\textsubscript{6} & [B]\\
		&t\textsubscript{9},t\textsubscript{10} & [E,B]\\
		&t\textsubscript{13} & [C,B]\\
		\{A\} &t\textsubscript{1} & [B,A]\\
		&t\textsubscript{3},t\textsubscript{11} & [C,E,A]\\
		&t\textsubscript{7} & [C,A]\\
    \end{tabular}
  \end{center}
\end{table}

\subsection{Incremental and Parallel Frequent itemsets mining}
Combining the previous 2 sections, yields an algorithm that does not rely on frequency order and uses parallelism advantages for computations of FIS.
The drawbacks are also drawn from the 2 algorithms - large memory consumption for saving all items and recursively calculating FIS. As we will show later in the \hyperref[sec:improvements]{Improvments} section, using an approach similar to ~\cite{kohefficient} and maintaining a pre-min support, together with using a semi-freq-order as in ~\cite{tanbeer2009efficient}, will significantly improve memory and mining runtime results.



\subsection{Song et al. }
\label{sec:song}
A paper by Song et el.~\cite{song2017} proposes 2 techniques for building and mining frequent itemsets - IncBuildingPFP and IncMiningPFP. IncBuildingPFP presents a parallel model based on CanTree that supports incremental mining. This approach is similar to IPFIM and presented in \autoref{sec:ipfim}.  


\subsubsection{IncMiningPFP}
As discussed by Song et el.~\cite{song2017} (and analysed later in this article as well), using CanTree as is, will result in memory and time limitation for relatively medium datasets (>1M) even with large clusters (>100G RAM).  IncMiningPFP solves the problem of mining by constructing FP-Growth tree for shards with new incremental items. For other shards, the data is taken from cache.

IncMiningPFP consists of the following steps:
\begin{steps}
	\item Group items G-list
	\item For the base case, for each shard, save the FIS and construct full FP-Trees to save all transactions. 
	\item For every Shard:
			\begin{enumerate}
			\item calculate and save frequency list per shard group -> F-list
			\item find 1-size FIS, extract paths from FP-Trees that contain only those items and build a FP-Growth tree.
			\item Extract full FIS from FP-Growth tree and save items in shard cache.
			\end{enumerate}
	\item For every iteration dD, devide based on G-List and send to the appropriate shards (similar to same as the distribution  stage at \autoref{alg:pfpalg}).
	\item For every shard: 
				\begin{enumerate}
				\item Recalculate F-list
				\item If got new items from dD, update tree with new values
				\item If this shard has added transactions, recalculate new FIS in similar way to initial stage and update shard cache. 
				\item Return Caches FIS.
				\end{enumerate}
	\item Reduce all previous FIS, similar to PFP.
\end{steps}

In this paper, although the IPFIM and IncBuildingPFP are similar, in \autoref{sec:improvements}, we present a different technique based on the CPTree~\cite{tanbeer2009efficient} and AFPIM~\cite{koh2004efficient} approach.

Song et el.~\cite{song2017} was also tested using classic map-reduce, while here we test the performance using Spark. Spark programs iteratively run about 100 times faster than Hadoop in-memory, and 10 times faster on disk ~\cite{spark}.

\subsubsection{Set-Cover}
The set cover problem is a classical question in combinatorics, computer science, operations research, and complexity theory. It is one of Karp's 21 NP-complete problems shown to be NP-complete in 1972 ~\cite{setcoveralgo}.

\paragraph{Formal Definition} Given a set system $ \Sigma = (X, S)$, where $S = \{s \textsubscript{1}, . . . , s\textsubscript{n}\}$, compute a set $C \subseteq \{1, . . . , n\}$ of minimum cardinality such that $X = \bigcup\limits_{i \in C} s \textsubscript{i}\\ $ 
For Example, given a set of elements $ \displaystyle \{1,2,...,n\}$ and a collection $X$ of $\displaystyle m$ sets whose union equals to $S$, the set cover problem is to identify the smallest sub-collection of $\displaystyle X$ whose union equals to $S$. For example, consider the universe $\displaystyle S=\{1,2,3,4,5\}$ and the collection of sets $\displaystyle C=\{\{1,2,3\},\{2,4\},\{3,4\},\{4,5\}\}$. Clearly the union of $\displaystyle C$ is $\displaystyle S$. However, we can cover all of the elements with the following, smaller number of sets: $\displaystyle X= \{\{1,2,3\},\{4,5\}\}$.
For our hypothesis, the Set-Cover-Ipfim algorithm [TODO:Ref set-cover-ipfim],we wanted to check whether the use of of optimal groups based on frequent itemsets, will result in better computation time, as it is potentially to optimize the distribution. A more detailed explanation is in [TODO].

Since the classic solution is NP-Hard, we used a naive greedy implementation, which requires $ O(mn^2) $ in ~\ref{alg:setcover}, where m is the item size, and n is number of items.

\begin{algorithm}
  \caption{Greedy set-cover implementation}\label{setcover}
  \begin{algorithmic}[1]
   \label{alg:setcover}
    \Procedure{Greedy-Set-Cover}{X,S}
      \State $U \gets X$  // $U$ stores the uncovered items
      \State $C \gets empty$ // $C$ stores the sets of the cover
      \While{$U$ is nonempty}
        \State select s[i] in $S$ that covers the most elements of $U$
		\State add i to $C$
		\State remove the elements of s[i] from $U$
      \EndWhile
 	\State return $C$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}
